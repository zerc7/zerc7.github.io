[{"title":"[参赛日记] 2025 ICPC 西安邀请赛","url":"/2025icpcxiani/","content":"省流：打铁。\n但是只差 5 名。\n但是罚时差了 60 分钟。\n\n这是我加的第一个队（上学期校赛没找到队友最后单挑拿了rk 9，其余都是单人赛），这是我们第一次参赛，这是我们第一次打铁，甚至 3 个人来自 2 个校区的 3 个学院且彼此在两个月前互不相识，只在网上知道某人某场比赛拿了 rank 几，能凑到一块是队友想打邀请赛，问我打不打，我觉得可以去，隧答应。\n感谢学长上早八，抢到了 3 个名额。\n一个月前 VP 了去年的邀请赛，4 题 Cu。\n Day -1 5.2\n坐火车过来的，因为学校不报销，能省则省吧。\n沿途好多山，真的好好看。\n提早一天到西安（由比赛顺便旅游变成了旅游顺便打比赛），先去了青龙寺（因为不要门票），里面好多佛像和中日友好的一些碑，然后发现佛祖也与时俱进，每个投币的地方都贴了二维码，支持赛博捐款（哭笑，然后去了大雁塔，当然没有上去，隔着老远拍了几张照。\n"},{"title":"[灯下闲谈] 用进废退 | AI","url":"/gossip1/","content":" 用进废退\n暂且不论其观点在生物演化方面的的对错，进探讨字面意思以及在生活中的表现。\n很早就听人说过一个人的智力巅峰是在高中，而我真正理解这句话是在上学期的期末考试（哭笑。现在回想的话，上了大学之后好像就很少再认认真真地学东西了，专业课前期勉强还能听得懂，到后面越来越抽象了，听的也就越来越少了（开始是听不懂，后来索性就不听了），公修课更不可能听的了，没有考试的压力，写个作文也有 AI 可以直接完成，再退一步还能中译英，反正不会真的动用脑子里本就不多的词汇去写，这也就导致我的英语水平是可预见性的急剧地下降。\n真正意识到说这件事是在视听说的课上，要求两个人对话，我发现我不会“说话”了，一方面是没有相关的经历，比方说旅游，我没有切实的经历我能说出来点什么呢，对吧，在一方面就是真的不会用英语遣词造句了，即使很简单的一句话，我都很难想到对应的词汇，即使说出来了，我也很怀疑某个介词是不是用错了，这句话是不是不太对啊，结构是不是很乱啊……总之，我是很能感受到这部分的差距的，而且是以一种“反指数（上下翻转）”的速度进行的，也就是前期衰退地还不明显，越到后期越严重，这是我对“废退”的感受。这警醒我必须做出改变。\n还有就是“用进”，艹，写到这的时候不知道咋往下写了，本来想反驳一下上面那句话的，但是发现我啥啥都“废退”了，是真的不如高中……\n算了，就这样吧。\n AI\n我印象里 ChatGPT 是在我上高二的时候兴起的，天天写关于 AI 的作文，讲怎么看待人工智能的发展，我当时是比较悲观的态度（并没有没写到试卷上），当然现在也是，我不喜欢用 AI 原因是 AI 会产生幻觉，即使使用正确的数据源去训练，仍然会产生失实的结果，更不用说数据源被污染了，所以我一直不相信 AI，对其生成的结果抱有怀疑的态度，这是其一。\n其二是 AI 对绝大数人来说是一个“更有智慧”的搜索引擎，AI 确实提高了人们查找信息的效率，但是我还是更习惯于传统的方式，习惯于自行查找，筛选信息，而不是被动地接受。\n三是“AI 味”。AI 生成的内容很有条理，但总觉得很奇怪，根本没有交谈的欲望，尤其是在评论区看到有人把 AI 生成的内容直接搬过来时，我都是直接略过，看不下去一点。\n以上其实都是感性方面的情绪化输出，客观来讲，AI 的进步很大，在很多方面真的算得上是解放生产力，但我认为还有很长的路要走，尤其在真实客观这方面。\n","tags":["灯下闲谈"]},{"title":"[CF] 1040 Div 2 部分题解","url":"/cf2130/","content":"博客又是好久没更了。。。\n挖坑不填。。。\n又挖了个新坑。。。\n\n题面\n题解\n\n A. Submission is All You Need\n\n给定非负整数可重集 SSS，重复任意次操作：选取子集 S′S'S′，将 sum(S′)\\text{sum}(S')sum(S′) 或 mex(S′)\\text{mex}(S')mex(S′) 添加到分数，删去 S′S'S′。求最大分数。\n\n只有当 S′={0}S'=\\lbrace 0\\rbraceS′={0} 时，mex(S′)\\text{mex}(S')mex(S′) 比 sum(S′)\\text{sum}(S')sum(S′) 更优。\n故 ans=∑S+Count(0)ans=\\sum S+\\text{Count(0)}ans=∑S+Count(0)\n B. Pathless\n\n给定数组 {a},ai∈{0,1,2}\\lbrace a\\rbrace, a_i\\in\\lbrace 0,1,2\\rbrace{a},ai​∈{0,1,2}（{a}\\lbrace a\\rbrace{a} 中包含至少一个 000，一个 111 和一个 222） 和整数 sss，是否可以通过重排 {a}\\lbrace a\\rbrace{a} 使得不存在一条从 111 到 nnn 的路径（每一步均可以向左或向右移动一个单位，但起点终点确定），若可以，输出任意满足条件的 {a}\\lbrace a\\rbrace{a}，不满足输出 −1-1−1。\n\n分类讨论：\n\n\n∑ai&gt;s\\sum a_i &gt; s∑ai​&gt;s：{a}\\lbrace a\\rbrace{a}\n\n\n∑ai=s\\sum a_i = s∑ai​=s：−1-1−1\n\n\n∑ai&lt;s\\sum a_i &lt; s∑ai​&lt;s：\n\n\ns−∑ai=1s - \\sum a_i = 1s−∑ai​=1：{0,…,0,2,…,2,1,…,1}\\lbrace 0,\\dots,0,2,\\dots,2,1,\\dots,1\\rbrace{0,…,0,2,…,2,1,…,1}\n\n\ns−∑ai&gt;1s - \\sum a_i &gt; 1s−∑ai​&gt;1：−1-1−1\n\n\n至少有一个 000 与 111 相邻：显然可以；\n\n\n000 只与 222 相邻：则至少有一个 111 与 222 相邻，可以在 ∑ai\\sum a_i∑ai​ 的基础上加上任意个 222 和 333 构成 sss。\n\n\n\n\n\n\n C. Double Perspective\n\n题面太长了不写了。。\n\ng(S)g(S)g(S) 一定为 000\n Sol 1\n用并查集维护，每次选不会构成环的加进答案。\n Sol 2\n被其他线段完全包起来的线段不选。\n D. Stay or Mirror\n\n给定一个排列 ppp，创建一个数组 a,ai∈{pi,2n−pi}a,a_i\\in\\lbrace p_i, 2n-p_i\\rbracea,ai​∈{pi​,2n−pi​}，求最小的相邻元素交换次数使 aaa 由大到小有序。复杂度要求：O(n2)O(n^2)O(n2)。\n\n对于 pi=1p_i=1pi​=1：\n\n\nStay：贡献为 n−in-in−i；\n\n\nMirror：贡献为 i−1i-1i−1；\n\n\n删去 111，重复此过程可得对于第 iii 个元素：\n\n\nStayi=∑j=i+1n1 if pj&gt;piStay_i=\\sum_{j=i+1}^n 1\\ \\text{if } p_j&gt;p_iStayi​=∑j=i+1n​1 if pj​&gt;pi​；\n\n\nMirrori=∑j=1i−11 if pj&gt;piMirror_i=\\sum_{j=1}^{i-1} 1\\ \\text{if } p_j&gt;p_iMirrori​=∑j=1i−1​1 if pj​&gt;pi​；\n\n\nans=∑i=1nmin⁡(Stayi,Mirrori)ans=\\sum_{i=1}^n\\min(Stay_i, Mirror_i)ans=∑i=1n​min(Stayi​,Mirrori​)\n E1. Interactive RBS (Easy Version)\n\nf(t)f(t)f(t) 是 ttt 中非空正则括号字串个数，通过 f(t)f(t)f(t) 找到长为 nnn 括号序列 sss，n≤1000,Q≤550n\\leq 1000,Q\\leq 550n≤1000,Q≤550。\n\n首先寻找 i,ji,ji,j 使得 si=s_i=si​= (，sj=s_j=sj​= )：\n\n\nf(s)≠0f(s)\\not=0f(s)=0：\n二分查找使得 f(s1s2…si)≠0f(s_1s_2\\dots s_i)\\not=0f(s1​s2​…si​)=0 的最小的右端点，则有 si−1=s_{i-1}=si−1​= (，si=s_i=si​= )；\n\n\nf(s)=0f(s)=0f(s)=0：\n由题意 sn=s_n=sn​= (，s1=s_1=s1​= )；\n\n\n构造 t=si)(sj()t={s}_i\\texttt{)(}{s}_j\\texttt{()}t=si​)(sj​()，可根据 f(t)f(t)f(t) 的结果知道 sis_isi​ 和 sjs_jsj​ 的值。\n","tags":["题解","CF"]},{"title":"[参赛日记] 2025 天梯赛","url":"/gplt2025/","content":"(100) + (25+25+25+17) + (2+20+0) = 214 = Ag-5\n没赛时代码，就不写题解了。\n 赛时\nL1-1 签到。\n然后服务器不出意料的炸了……\nL1-3（高温补贴）就几个判断，但是题面写的真的很烂，写一半就又重写了一遍……写完发现交不上去……就一直刷新下一题的题面……\nL1-6（这不是字符串题）看到 262626，直接用 string，代码写的特别丑，边界啥的懒得算，是 i 还是 i+1，直接根据样例调，这个题还挺费时间的。\n写到了 L1-6 外加 L1-8，发现能交了，全交上去了，看着由 Waiting 一个个变成 Accepted，然后写 L1-7（大幂数），觉得挺难，但直接写暴力就能过（这个题面也是没讲明白，但不太影响做题）。\n写完 L1 应该是用了 1h。\nL2-1（算式拆解）开始想用栈做，发现不用这么麻烦，先找到第一个出现的 )，从此位置向左找到第一个 (，用 substr 输出答案，然后把输出的内容连同左右括号从原字符串中删去，不断重复这个过程即可。\nL2-2（三点共线）只看了 nnn 的数据范围，心想怎么这么难，想了半天，往后一看，y∈0,1,2y\\in \\\\{0,1,2\\\\}y∈0,1,2……先枚举 y=1y=1y=1 的，再枚举 y=0y=0y=0 的，y=2y=2y=2 在 set 里查找（赛时好像用的是 bitset，O(1)O(1)O(1) 查找），记得输入后用 unique 去重。\nL2-3（胖达的山头）题面又是不清不楚，实际上用差分标记每个熊猫从第几秒占到第几秒，求个最大值就完了。\nL2-4（被n整除的n位数）用循环写的暴力，然后用 goto 优化了一下，某一位不满足就对这一位 +1，17pts，1 WA 1 TLE。\n到这应该是 2h，期间监考说时间延长了 1h。\nL3-1（人生就像一场旅行）赛时读错题了（理解错意思了）……离二等就差这个题了……\nL3-2（影响力）想出来正解了，但被 nm≤106nm\\leq 10^6nm≤106 搞得代码写得很乱，到最后也没调出来……最后 3min 才写暴力，拿了 20pts\nL3-3（污染大亨）没看。\n 赛后复盘\n刚打完其实没啥，但今天（4.22）复盘的时候特别无语又很遗憾，尤其是 L3-1,2\n 经验\n还是太菜，加训加训\n"},{"title":"[机器学习] 学习笔记 壹 基本概念","url":"/ml1/","content":"李宏毅 Machine Learning (2021 Spring)\n 什么是机器学习\nMachine Learning ~= Looking for Function\n 机器学习可以做什么：\n\n\nSpeech Recognition\n\n\nImage Recognition\n\n\nPlaying Go (AlphaGo)\n\n\n…\n\n\n 类别 Types：\n\n\n回归 Regression: The func outputs a scalar\n\n\n分类 Classification: Given options(classes), outputs the correct one\ne.g. AlphaGo\n\n\nStructured Learning: create sth. with structure(img, doc)\n\n\n 怎么找这样的一个 Func\n例子：YouTube 观看人数预测。\n 1. Func with Uk Parameters\ny=b+wx1y = b + wx_1y=b+wx1​: Model;\nx1x_1x1​: feature;\nwww: weight 权重;\nbbb: bias 偏移量;\n 2. Define Loss from Training Data\nL(b,w)L(b, w)L(b,w): Loss is a func of param\nLoss: How good a set of value is.\nL=1N∑nenL=\\frac{1}{N}\\sum_n e_n\nL=N1​n∑​en​\nMAE: Mean absolute error e=∣y−y^∣e=|y-\\hat y|e=∣y−y^​∣\nMSE: Mean square error  e=(y−y^)2e=(y-\\hat y)^2e=(y−y^​)2\nCross-entropy 交叉熵: if yyy and y^\\hat yy^​ are both probability distributions\nError Surface 误差曲面: 不同的 bbb 和 www 计算出的 Loss 绘成的等高线图。\n 3. Optimization\nw∗,b∗=ai=argmin⁡w,bLw^*, b^* = ai=arg \\min_{w, b} Lw∗,b∗=ai=argminw,b​L (argmin⁡arg \\minargmin: 使式子达到最小值时变量的取值)\nGradient Descent 梯度下降:\n 假设只有一个 UkPm w\nRandomly Pick an inital value w0w_0w0​\nCompute ∂L∂w∣w=w0\\frac{\\partial L}{\\partial w}|_{w=w_0}∂w∂L​∣w=w0​​\n\n\nNegative =&gt; Increase www\n\n\nPositive =&gt; Decrease www\n步伐大小: η∂L∂w∣w=w0\\eta\\frac{\\partial L}{\\partial w}|_{w=w_0}η∂w∂L​∣w=w0​​\nη\\etaη: Learning rate, HyperParameters\nw1←η∂L∂w∣w=w0w_1\\leftarrow \\eta\\frac{\\partial L}{\\partial w}|_{w=w_0}w1​←η∂w∂L​∣w=w0​​\n\n\nUpdate www iteratively\n什么时候停下来\n\n\n达到设定的次数 epoch(HyperParam)\n\n\n理想情况：微分值为 0\n\n\nGD 的问题：\nLocal minima \\ Global minima\nLM 是假问题？\n 推广到两个参数\nRandomly Pick inital values w0w_0w0​, b0b_0b0​\nCompute:\n∂L∂w∣w=w0,b=b0\\frac{\\partial L}{\\partial w}|_{w=w_0, b=b_0}∂w∂L​∣w=w0​,b=b0​​\n∂L∂b∣w=w0,b=b0\\frac{\\partial L}{\\partial b}|_{w=w_0, b=b_0}∂b∂L​∣w=w0​,b=b0​​\nw1←η∂L∂w∣w=w0,b=b0w_1\\leftarrow \\eta\\frac{\\partial L}{\\partial w}|_{w=w_0, b=b_0}w1​←η∂w∂L​∣w=w0​,b=b0​​\nb1←η∂L∂b∣w=w0,b=b0b_1\\leftarrow \\eta\\frac{\\partial L}{\\partial b}|_{w=w_0, b=b_0}b1​←η∂b∂L​∣w=w0​,b=b0​​\nUpdate www bbb iteratively\nNew Model: y=b+∑j=17wjxjy=b+\\sum_{j=1}^{7}w_jx_jy=b+∑j=17​wj​xj​, 将前 7 天的数据纳入考虑\n Step 1. Model(Function with unknown)\nLinear models have severe limitation(Model bias), so we need a more flexible model.\nAll Piecewise Linear Curves = constant + sum of a set of ReLU\nBeyond Piecewise Curves + 取点 = constant + sum of a set of ReLU\n Sigmoid Function\ny=c11+e−(b+wx1)=c sigmoid(b+wx1)y=c\\frac{1}{1+e^{-(b+wx_1)}}=c\\ \\text{sigmoid}(b+wx_1)\ny=c1+e−(b+wx1​)1​=c sigmoid(b+wx1​)\n More flexible function\ny=b+∑ici sigmoid(bi+wix1)y=b+\\sum_ic_i\\ \\text{sigmoid}(b_i+w_ix_1)\ny=b+i∑​ci​ sigmoid(bi​+wi​x1​)\ny=b+∑ici sigmoid(bi+∑jwijxj)y=b+\\sum_ic_i\\ \\text{sigmoid}(b_i+\\sum_jw_{ij}x_j)\ny=b+i∑​ci​ sigmoid(bi​+j∑​wij​xj​)\nr=b+Wx\\boldsymbol{r} = \\boldsymbol{b}+W\\boldsymbol{x} \nr=b+Wx\na=σ(r)\\boldsymbol a=\\sigma(\\boldsymbol r)\na=σ(r)\ny=b+cTay=b+\\boldsymbol c^T \\boldsymbol a\ny=b+cTa\ny=b+cTσ(b+Wx)y=b+\\boldsymbol c^T \\sigma(\\boldsymbol{b}+W\\boldsymbol{x})\ny=b+cTσ(b+Wx)\nxxx: Feature\nW,b,cT,bW, \\boldsymbol{b}, \\boldsymbol c^T, bW,b,cT,b: Unknown Parameters =&gt; θ\\boldsymbol \\thetaθ\n Step 2. Define loss from training data\nL(θ)L(\\boldsymbol \\theta)L(θ)\n Step 3. Optimization\nθ∗=argmin⁡θL\\boldsymbol \\theta^*=arg\\min_{\\boldsymbol \\theta}Lθ∗=argminθ​L\nRandomly Pick inital values θ0\\boldsymbol \\theta^0θ0\nCompute gradient:\ng=[∂L∂θ1∣θ=θ0∂L∂θ1∣θ=θ0⋮]=∇L(θ0)\\boldsymbol g=\n\\begin{bmatrix}\n \\frac{\\partial L}{\\partial \\theta_1}|_{\\boldsymbol \\theta=\\boldsymbol \\theta^0}\\\\\\\\\n \\frac{\\partial L}{\\partial \\theta_1}|_{\\boldsymbol \\theta=\\boldsymbol \\theta^0}\\\\\\\\\n \\vdots\n\\end{bmatrix}\n=\\nabla L(\\boldsymbol\\theta^0)\ng=⎣⎢⎢⎢⎢⎢⎢⎡​∂θ1​∂L​∣θ=θ0​∂θ1​∂L​∣θ=θ0​⋮​⎦⎥⎥⎥⎥⎥⎥⎤​=∇L(θ0)\nw1←η∂L∂w∣w=w0,b=b0w_1\\leftarrow \\eta\\frac{\\partial L}{\\partial w}|_{w=w_0, b=b_0}w1​←η∂w∂L​∣w=w0​,b=b0​​\nUpdate θ\\boldsymbol\\thetaθ iteratively\n Mini-batch 梯度下降\nNNN 笔资料，随机分为若干个 batch，取一个 batch 计算 L1L^1L1，θ1←θ0−η(g=∇L1(θ0))\\boldsymbol\\theta^1\\leftarrow\\boldsymbol\\theta^0-\\eta(\\boldsymbol g=\\nabla L^1(\\boldsymbol\\theta^0))θ1←θ0−η(g=∇L1(θ0))\n…\nEpoch: 把所有 batch 看过一遍\nUpdate: 每一次更新参数\nHard Sigmoid &lt;== Rectified Linear Unit (ReLU) * 2\nReLU: cmax⁡(0,b+wx1)c\\max(0, b+wx_1)cmax(0,b+wx1​)\ny=b+∑2ici max⁡(0,bi+∑jwijxj)y=b+\\sum_{2i}c_{i}\\ \\max(0, b_i+\\sum_jw_{ij}x_j)\ny=b+2i∑​ci​ max(0,bi​+j∑​wij​xj​)\nActivation Function 激活函数: Sigmoid and ReLU\nNeural Network\nDeep Learning: Many layers means deep\nOverfitting: Batter on training data, worse on unseen data\n Backpropagation 反向传播\nTo compute the gradients efficiently.\n Chain Rule\nCase 1:\ny=g(x),z=h(y)y=g(x), z=h(y)\ny=g(x),z=h(y)\nΔx→Δy→Δz\\Delta x\\to\\Delta y\\to \\Delta z\nΔx→Δy→Δz\ndzdx=dzdydydx\\frac{\\text d z}{\\text d x} = \\frac{\\text d z}{\\text d y}\\frac{\\text d y}{\\text d x}\ndxdz​=dydz​dxdy​\nCase 2:\nx=g(s),y=h(s),z=k(x,y)x=g(s), y=h(s), z=k(x,y)\nx=g(s),y=h(s),z=k(x,y)\nΔs→ΔxΔy→Δz\\Delta s\\to \\begin{matrix} \\Delta x\\\\\\\\ \\Delta y \\end{matrix} \\to \\Delta z\nΔs→ΔxΔy​→Δz\ndzds=∂z∂xdxds+∂z∂ydyds\\frac{\\text d z}{\\text d s} = \\frac{\\partial z}{\\partial x}\\frac{\\text d x}{\\text d s} + \\frac{\\partial z}{\\partial y}\\frac{\\text d y}{\\text d s}\ndsdz​=∂x∂z​dsdx​+∂y∂z​dsdy​\nL(θ)=∑n=1NCn(θ)→∂L(θ)∂w=∑n=1N∂Cn(θ)∂wL(\\theta)=\\sum_{n=1}^{N}C^n(\\theta)\\to \\frac{\\partial L(\\theta)}{\\partial w}=\\sum_{n=1}^{N}\\frac{\\partial C^n(\\theta)}{\\partial w}\nL(θ)=n=1∑N​Cn(θ)→∂w∂L(θ)​=n=1∑N​∂w∂Cn(θ)​\n取出第一个 neuron，z=w1x1+w2x2+bz=w_1x_1+w_2x_2+bz=w1​x1​+w2​x2​+b\n∂C∂w=∂z∂w∂C∂z\\frac{\\partial C}{\\partial w} = \\frac{\\partial z}{\\partial w} \\frac{\\partial C}{\\partial z}\n∂w∂C​=∂w∂z​∂z∂C​\nCompute ∂z/∂w\\partial z/\\partial w∂z/∂w:\n∂z∂w1=x1,∂z∂w2=x2\\frac{\\partial z}{\\partial w_1} = x_1,\n\n\\frac{\\partial z}{\\partial w_2} = x_2\n∂w1​∂z​=x1​,∂w2​∂z​=x2​\n听不懂了……\n","tags":["机器学习"]},{"title":"[机器学习] 作业 壹 2022Spring-hw1 COVID-19 Cases Prediction","url":"/mlwork1/","content":" Download Data\n!gdown https://drive.google.com/uc?id=1kLSW_-cW2Huj7bh84YTdimGBOJaODiOS --output covid.train.csv!gdown https://drive.google.com/uc?id=1iiI5qROrAhZn-o4FPqsE97bMzDEFvIdg --output covid.test.csv\nData: 118 features (id + 37 + 16 * 5, label included)\n Import Packages\nimport torchimport torch.nn as nnfrom torch.utils.data import DataLoader, Dataset, random_splitfrom torch.utils.tensorboard import SummaryWriterimport numpy as npimport mathimport pandas as pdimport csvimport osimport matplotlib.pyplot as pltfrom matplotlib.pyplot import figure\n Same Seed\n保证实验的可重复性\ndef same_seed(seed):    # 使用确定性的卷积算法    torch.backends.cudnn.deterministic = True    # 禁止为卷积层选择最优算法    torch.backends.cudnn.benchmark = False    np.random.seed(seed)    torch.manual_seed(seed)    if torch.cuda.is_available():        torch.cuda.manual_seed(seed)\n Dataset\ndef data_set_split(data_set, valid_ratio, seed):    valid_data_size = int(len(data_set) * valid_ratio)    train_data_size = len(data_set) - valid_data_size    train_set, valid_set = random_split(data_set, [train_data_size, valid_data_size], generator=torch.Generator().manual_seed(seed))    return np.array(train_set), np.array(valid_set)def select_feature(train_set, valid_set, test_set):    label_train_set = train_set[:, -1]    label_valid_set = valid_set[:, -1]    feature_train_set = train_set[:, :-1]    feature_valid_set = valid_set[:, :-1]    feature_test_set = test_set    return feature_train_set, feature_valid_set, feature_test_set, label_train_set, label_valid_setclass COVID19Dataset(Dataset):    def __init__(self, features, targets=None):        if targets is None:            self.targets = targets        else:            self.targets = torch.FloatTensor(targets)        self.features = torch.FloatTensor(features)    def __getitem__(self, idx):        if self.targets is None:            return self.features[idx]        else:            return self.features[idx], self.targets[idx]    def __len__(self):        return len(self.features)\n Neural Network Model\nclass my_model(nn.Module):    def __init__(self, input_dim):        super(my_model, self).__init__()        self.layers = nn.Sequential(            nn.Linear(input_dim, 64),            nn.ReLU(),            # nn.Linear(64, 64),            # nn.ReLU(),            # nn.Linear(64, 64),            # nn.ReLU(),            # nn.Linear(64, 64),            # nn.ReLU(),            # nn.Linear(64, 32),            # nn.ReLU(),            nn.Linear(64, 1),        )    def forward(self, x):        x = self.layers(x)        x = x.squeeze(1)        return x\n Hyper Parameters\ndevice = &#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;config = &#123;    &#x27;seed&#x27;: 7,    &#x27;valid_ratio&#x27;: 0.2,    &#x27;n_epochs&#x27;: 5000,    &#x27;batch_size&#x27;: 256,    &#x27;lr&#x27;: 1e-5,    &#x27;early_stop&#x27;: 800,    &#x27;save_path&#x27;: &#x27;./models/model.ckpt&#x27;&#125;\n Training Progress\nloss_record_train = []loss_record_valid = []\ndef train(train_loader, valid_loader, model, config, device):    criterion = nn.MSELoss(reduction=&#x27;mean&#x27;)    optimizer = torch.optim.SGD(model.parameters(), lr=config[&#x27;lr&#x27;], momentum=0.9)    writer = SummaryWriter()    if not os.path.exists(&#x27;./models&#x27;):        os.mkdir(&#x27;./models&#x27;)    n_epochs = config[&#x27;n_epochs&#x27;]    best_loss = math.inf    step = 0    early_stop_count = 0    for epoch in range(n_epochs):        model.train()        loss_record = []        for x, y in train_loader:            optimizer.zero_grad()            x, y = x.to(device), y.to(device)            pred = model(x)            loss = criterion(pred, y)            loss.backward()            optimizer.step()            step += 1            loss_record.append(loss.detach().item())            loss_record_train.append(loss.detach().item())        mean_train_loss = sum(loss_record) / len(loss_record)        writer.add_scalar(&#x27;loss/train&#x27;, mean_train_loss, step)        model.eval()        loss_record = []        for x, y in valid_loader:            x, y = x.to(device), y.to(device)            with torch.no_grad():                pred = model(x)                loss = criterion(pred, y)            loss_record.append(loss.item())        mean_valid_loss = sum(loss_record) / len(loss_record)        loss_record_valid.append(mean_valid_loss)        writer.add_scalar(&#x27;loss/valid&#x27;, mean_valid_loss, step)        print(f&#x27;Epoch [&#123;epoch+1&#125;/&#123;n_epochs&#125;]: Train loss: &#123;mean_train_loss:.4f&#125;, Valid loss: &#123;mean_valid_loss:.4f&#125;]&#x27;)        if mean_valid_loss &lt; best_loss:            best_loss = mean_valid_loss            torch.save(model.state_dict(), config[&#x27;save_path&#x27;])            print(f&#x27;Saving model[&#123;epoch+1&#125;/&#123;n_epochs&#125;] with valid loss &#123;best_loss:.3f&#125;&#x27;)            early_stop_count = 0        else:            early_stop_count += 1        if early_stop_count &gt; config[&#x27;early_stop&#x27;]:            print(&#x27;Early stopping&#x27;)            break\n Prepare\nsame_seed(config[&#x27;seed&#x27;])train_data = pd.read_csv(&#x27;./covid.train.csv&#x27;).drop(columns=[&#x27;id&#x27;]).valuestest_set = pd.read_csv(&#x27;./covid.test.csv&#x27;).drop(columns=[&#x27;id&#x27;]).valuestrain_set, valid_set = data_set_split(train_data, config[&#x27;valid_ratio&#x27;], config[&#x27;seed&#x27;])feature_train, feature_valid, feature_test, label_train, label_valid = select_feature(train_set, valid_set, test_set)train_dataset = COVID19Dataset(feature_train, label_train)valid_dataset = COVID19Dataset(feature_valid, label_valid)test_dataset = COVID19Dataset(feature_test)train_loader = DataLoader(train_dataset, batch_size=config[&#x27;batch_size&#x27;], shuffle=True)valid_loader = DataLoader(valid_dataset, batch_size=config[&#x27;batch_size&#x27;], shuffle=True)test_loader = DataLoader(test_dataset, batch_size=config[&#x27;batch_size&#x27;], shuffle=False)\n Start\nmodel = my_model(input_dim=feature_train.shape[1]).to(device)train(train_loader, valid_loader, model, config, device)\n Plot Learning Curve\ndef plot_learning_curve():    total_steps = len(loss_record_train)    x_1 = range(total_steps)    x_2 = x_1[::len(loss_record_train) // len(loss_record_valid)]    figure(figsize=(6, 4))    plt.plot(x_1, loss_record_train, c=&#x27;tab:red&#x27;, label=&#x27;train&#x27;)    plt.plot(x_2, loss_record_valid, c=&#x27;tab:cyan&#x27;, label=&#x27;mean valid&#x27;)    plt.ylim(0.0, 5.)    plt.xlabel(&#x27;Training steps&#x27;)    plt.ylabel(&#x27;MSE loss&#x27;)    plt.title(&#x27;Learning curve of COVID-19 model&#x27;)    plt.legend()    plt.show()plot_learning_curve()\n\n Predict\ndef predict(test_set, model, config):    model.eval()    preds = []    for x in test_loader:        x = x.to(device)        with torch.no_grad():            pred = model(x)            preds.append(pred.detach().cpu())    preds = torch.cat(preds, dim=0).numpy()    return predsdef save_pred(preds, file):    with open(file, &#x27;w&#x27;) as fp:        writer = csv.writer(fp)        writer.writerow([&#x27;id&#x27;, &#x27;tested_positive&#x27;])        for i, p in enumerate(preds):            writer.writerow([i, p])model = my_model(input_dim=feature_train.shape[1]).to(device)model.load_state_dict(torch.load(config[&#x27;save_path&#x27;]))preds = predict(test_loader, model, config)save_pred(preds, &#x27;./pred.csv&#x27;)\n","tags":["机器学习"]},{"title":"[机器学习] 学习笔记 贰 训练不起来怎么办","url":"/ml2/","content":"\n Model Bias\nProblem: The model is too simple.\nSolution: Redesign you model to make it more flexible.\nMore features or Deep Learning (more neurons, layers)\n Optimization\nCritical point (gradient is close to 0): Local minima / Saddle point\nHHH: Hessian Matrix\nL(θ)≈L(θ′)+12(θ−θ′)TH(θ−θ′)L(\\boldsymbol\\theta)\\approx L(\\boldsymbol\\theta&#x27;)+\\frac{1}{2}(\\boldsymbol\\theta-\\boldsymbol\\theta&#x27;)^{T}H(\\boldsymbol\\theta-\\boldsymbol\\theta&#x27;)\nL(θ)≈L(θ′)+21​(θ−θ′)TH(θ−θ′)\n记 vTHv=12(θ−θ′)TH(θ−θ′)\\boldsymbol v^THv=\\frac{1}{2}(\\boldsymbol\\theta-\\boldsymbol\\theta&#x27;)^{T}H(\\boldsymbol\\theta-\\boldsymbol\\theta&#x27;)vTHv=21​(θ−θ′)TH(θ−θ′)\n\n\nLocal minima:\nFor all v\\boldsymbol vv, vTHv&gt;0→Around θ′:L(θ)&gt;L(θ′)\\boldsymbol v^THv&gt;0\\to \\text{Around } \\boldsymbol\\theta&#x27;:L(\\boldsymbol\\theta)&gt;L(\\boldsymbol\\theta&#x27;)vTHv&gt;0→Around θ′:L(θ)&gt;L(θ′)\nHHH is positive definite = All eigen values are positive.\n\n\nLocal maxima:\nFor all v\\boldsymbol vv, vTHv&lt;0→Around θ′:L(θ)&lt;L(θ′)\\boldsymbol v^THv&lt;0\\to \\text{Around } \\boldsymbol\\theta&#x27;:L(\\boldsymbol\\theta)&lt;L(\\boldsymbol\\theta&#x27;)vTHv&lt;0→Around θ′:L(θ)&lt;L(θ′)\nHHH is negative definite = All eigen values are negative.\n\n\nSaddle point:\nSometimes vTHv&gt;0\\boldsymbol v^THv&gt;0vTHv&gt;0, Sometimes vTHv&lt;0\\boldsymbol v^THv&lt;0vTHv&lt;0\nSome eigen values are positive, and some are negative.\n\n\nerror surface 维度很高，因此 local minima 很少。\n Model Bias v.s. Optimization Issue\ne.g. 56-layer 比 20-layer 更差，不是 overfitting，而是 optimization issue。\n建议：先使用简单的 model，不容易出现 optimization issue，\n Overfitting\nAn extreme example:\nf(x)={y^i∃xi=xrandomotherwisef(\\boldsymbol x)=\n\\begin{cases}\n  \\hat y^i &amp; \\exist\\boldsymbol x^i=\\boldsymbol x\\\\\n  \\text{random}  &amp; \\text{otherwise}\n\\end{cases}\nf(x)={y^​irandom​∃xi=xotherwise​\nSolution:\n\n\nMore training data\n\n\nData augumentation\ne.g. 图像识别：左右翻转，缩放，但不应该上下翻转\n\n\nConstrained model\n\n\nLess parameters, sharing parameters (CNN)\n\n\nLess features\n\n\nEarly stopping\n\n\nRegularization 正则化\n\n\nDropout\n\n\n\n\n Batch\n\n\nLarge batch: long time for cooldown, but powerful\n\n\nSamll batch: short time for cooldown, but noisy\n\n\n因为有 GPU 平行运算（显存换时间），所以跑一个 epoch，large batch update 的次数更少，更有效率\nWarning: Large batch size 容易出现 optimization fails\n\n\n\n\nSamll\nLarge\n\n\n\n\nSpeed for one update (no parallel)\nFaster\nSlower\n\n\nSpeed for one update (with parallel)\nSame\nSame(not too large)\n\n\nTime for one epoch\nSlower\nFaster\n\n\nGradient\nNosiy\nStable\n\n\nOptimization\nBetter\nWorse\n\n\nGeneralization\nBetter\nWorse\n\n\n\n Momentum\n Learning Rate\n震荡，Loss 不再减少，但是 Gradient 不为 0\nLearning rate 要为每一个参数客制化\nθit+1←θit−ηgit,git=∂L∂θi∣θ=θt\\theta_i^{t+1}\\leftarrow\\theta_i^t-\\eta g_i^t, g_i^t=\\frac{\\partial L}{\\partial \\theta_i}|_{\\theta=\\theta^t}\nθit+1​←θit​−ηgit​,git​=∂θi​∂L​∣θ=θt​\nθit+1←θit−ησitgit\\theta_i^{t+1}\\leftarrow\\theta_i^t-\\frac{\\eta}{\\sigma_i^t} g_i^t\nθit+1​←θit​−σit​η​git​\n Adagrad\nRoot Mean Square 均方根\nθi1←θi0−ησi0gi0, σi0=(gi0)2=∣gi0∣θi2←θi1−ησi1gi1, σi1=12[(gi0)2+(gi1)2]…θit+1←θit−ησitgit, σit=1t+1∑j=0t(gij)2\\theta_i^{1}\\leftarrow\\theta_i^0-\\frac{\\eta}{\\sigma_i^0} g_i^0,\\ \\sigma_i^0=\\sqrt{(g_i^0)^2}=|g_i^0|\\\\\\\\\n\n\\theta_i^{2}\\leftarrow\\theta_i^1-\\frac{\\eta}{\\sigma_i^1} g_i^1,\\ \\sigma_i^1=\\sqrt{\\frac{1}{2}[(g_i^0)^2 + (g_i^1)^2]}\\\\\\\\\n\n\\dots\\\\\\\\\n\n\\theta_i^{t+1}\\leftarrow\\theta_i^t-\\frac{\\eta}{\\sigma_i^t} g_i^t,\\ \\sigma_i^t=\\sqrt{\\frac{1}{t+1}\\sum_{j=0}^t(g_i^j)^2}\nθi1​←θi0​−σi0​η​gi0​, σi0​=(gi0​)2​=∣gi0​∣θi2​←θi1​−σi1​η​gi1​, σi1​=21​[(gi0​)2+(gi1​)2]​…θit+1​←θit​−σit​η​git​, σit​=t+11​j=0∑t​(gij​)2​\n RMSProp\n0&lt;α&lt;10&lt;\\alpha&lt;10&lt;α&lt;1\nθi1←θi0−ησi0gi0, σi0=(gi0)2θi2←θi1−ησi1gi1, σi1=α(σi0)2+(1−α)(gi1)2…θit+1←θit−ησitgit, σit=α(σit−1)2+(1−α)(git)2\\theta_i^{1}\\leftarrow\\theta_i^0-\\frac{\\eta}{\\sigma_i^0} g_i^0,\\ \\sigma_i^0=\\sqrt{(g_i^0)^2}\\\\\\\\\n\n\\theta_i^{2}\\leftarrow\\theta_i^1-\\frac{\\eta}{\\sigma_i^1} g_i^1,\\ \\sigma_i^1=\\sqrt{\\alpha(\\sigma_i^0)^2 + (1-\\alpha)(g_i^1)^2}\\\\\\\\\n\n\\dots\\\\\\\\\n\n\\theta_i^{t+1}\\leftarrow\\theta_i^t-\\frac{\\eta}{\\sigma_i^t} g_i^t,\\ \\sigma_i^t=\\sqrt{\\alpha(\\sigma_i^{t-1})^2 + (1-\\alpha)(g_i^t)^2}\nθi1​←θi0​−σi0​η​gi0​, σi0​=(gi0​)2​θi2​←θi1​−σi1​η​gi1​, σi1​=α(σi0​)2+(1−α)(gi1​)2​…θit+1​←θit​−σit​η​git​, σit​=α(σit−1​)2+(1−α)(git​)2​\nAdam: RMSProp + Momentum\n Learning Rate Scheduling\nθit+1←θit−ηtσitgit\\theta_i^{t+1}\\leftarrow\\theta_i^t-\\frac{\\eta^t}{\\sigma_i^t} g_i^t\nθit+1​←θit​−σit​ηt​git​\nLearning Rate Dacay\n递减\nWarm Up\n先增大再减小\n","tags":["机器学习"]},{"title":"一段新的开始","url":"/new/","content":"其实刚上大学那会是想继续写的，但是经历的事情还少，也没啥值得说的，而且也很少把时间投到算法竞赛里面，更没有要写的了。\n但是下学期开始有了更多的比赛，尤其是四月中旬的天梯赛，五月初的西安邀请赛，都“提醒”或者说督促我花更多的时间到这里面，这段时间也确实做了一些好题，是值得反复品味的好题，我认为做过之后没有及时地写成题解再整理一遍思路，留个档是非常可惜的，所以就有了更博客的想法。\n我之前是用 Hexo 搭过博客的，最早应该是初中（大概是初三？记不清了），当时图的新鲜，好看的主题都想试一试，但是内容是没有的。然后是高二，当时浅浅的接触了 OI，确实是有用博客写题解和学习笔记的需求的，所以说当时写了不少的文章，但因为当时的我有种急功近利的感觉，好多算法和题其实并没有理解得很清楚，所以那时候的内容是多但没有多少营养的。希望从现在开始，能真正写出有营养，有价值的内容。\n然后就是博客的主题，我不是很喜欢花哨的东西，但是特别简洁的又很素，所以选了这个主题。挑的时候好多的主题的演示网站都 404 了，心里莫名地有些伤感，回看我之前写的文章也是，有种物是人非的感觉。\n"},{"title":"[VP] 2024 ICPC 西安邀请赛","url":"/vp2024icpcxiani/","content":"题目(Luogu): [ICPC2024 Xi’an I] ICPC2024 邀请赛西安站重现赛\nPDF(适于打印): The 2024 ICPC China Shaanxi National Invitational Programming Contest\n注意：PDF 版题目由本人根据 Luogu 题面使用 LaTeX\\LaTeXLATE​X 制作，不是官方版本，如果你需要在此基础上更改，请下载 源Tex文件（本人经验有限，诸多部分是边学边做，故显凌乱）。\nUPD: 当时的我还不知道 Polygon\n以下就按照 A 题顺序来讲吧：\n Problem J. Triangle\n\n给你一个两个直角边分别在 x,yx, yx,y 轴，直角顶点在原点的直角三角形，求它占了几个格子（一个格子如果有 ≥12\\geq\\frac{1}{2}≥21​ 的面积在三角形里面，则被称为所占的格子）1≤a,b≤1061\\leq a, b \\leq 10^61≤a,b≤106\n\n可以看到数据范围并不大，因此我们可以设斜边方程为 y=−bax+by=-\\frac{b}{a}x+by=−ab​x+b，沿着 xxx 轴（或 yyy 轴）扫描，记录每一纵列占用多少格子，事实上，我们只需要知道 y(x−0.5)y(x-0.5)y(x−0.5) 的值并上取整即可。\n注意此题卡精度（照我的写法是要开 long double 的）。\nlong double a, b;int main() {    for(int i = 1; i &lt;= a; i++)        ans += (1 - (i - 0.5) / a) * b + 0.5;}\n Problem M. Chained Lights\n\nvoid press(int x){    light[x]^=1;    for (int y=x+x;y&lt;=n;y+=x) press(y);}for (int i=1;i&lt;=n;i++) press(i);\n优化上述代码。1≤T≤105,1≤k≤n≤1061\\leq T \\leq 10^5, 1\\leq k \\leq n \\leq 10^61≤T≤105,1≤k≤n≤106\n\n纯纯的诈骗题。\n首先 T,nT, nT,n 都是没用的，其次 “After all the operations, lights 1, 2, 3 will be turned on and light 4 is still off.” 太有误导性了，我原以为这是所有操作都执行完的结果，实际上只是 i=1i=1i=1 执行完了……然后玩几个样例就能看出来除了 111 其余都是 No……\n很明显能看出来这是和因数有关的，不妨分类讨论：素数(p)： 只会在 press(1) 和 press(p) 的时候操作两次，所以灯的状态不变；合数： 除开 111 和它本身，剩余的因子都会使它被操作两次，灯的状态也不变。\nputs(k == 1 ? \"YES\" : \"NO\");\n Problem F. XOR Game\n博弈论。\n题面挺抽象的，读了好几遍才读懂（也有可能是英语差。\n首先：对于一个数 Alice 先手，如果有奇数个，Alice 总可得到（以下简称达到目的为赢），有偶数个，则 Bob 赢，这还是比较显然的。\n","tags":["题解","VP"]},{"title":"[参赛日记] 筑梯杯 郑州轻工业大学 第十七届程序设计大赛（牛客线上同步赛）","url":"/zzuli17/","content":"\n"},{"title":"赛时 VSCode 简明配置教程","url":"/vscode-config-for-xcpc/","content":"PDF Version\n赛时 VSCode 可能不包含任何插件，因此在较短时间内配置好 VSCode 的编译运行指令是很有必要的（虽然我觉得没有任何插件的 VSCode 和 DEV-C++ 基本没有区别）。\n在工作文件夹下新建文件夹 .vscode，新建文件 tasks.json。\n运行程序时可以从同目录下的 sample.in 读入数据并输出到 sample.out。\n可将 .exe 替换为 $&#123;fileBasenameNoExtension&#125;。\n&#123;    &quot;version&quot;: &quot;2.0.0&quot;,    &quot;tasks&quot;: [        &#123;            &quot;label&quot;: &quot;build&quot;,            &quot;type&quot;: &quot;shell&quot;,            &quot;command&quot;: &quot;cls; g++ &#x27;$&#123;file&#125;&#x27; -std=c++14 -Wall -o .exe&quot;        &#125;,        &#123;            &quot;label&quot;: &quot;run&quot;,            &quot;type&quot;: &quot;shell&quot;,            &quot;command&quot;: &quot;cls; Get-Content ./sample.in | ./.exe &gt; ./sample.out&quot;        &#125;,        &#123;            &quot;label&quot;: &quot;buildrun&quot;,            &quot;type&quot;: &quot;shell&quot;,            &quot;command&quot;: &quot;cls; g++ &#x27;$&#123;file&#125;&#x27; -std=c++14 -Wall -o .exe; Get-Content ./sample.in | ./.exe &gt; ./sample.out&quot;        &#125;    ]&#125;\n配置快捷键，shift+numpad_add 设为字体放大，f9 - f11 依次为编译，运行，编译并运行（老 DEV 了）。\n[    &#123;        &quot;key&quot;: &quot;shift+numpad_add&quot;,        &quot;command&quot;: &quot;editor.action.fontZoomIn&quot;    &#125;,    &#123;        &quot;key&quot;: &quot;shift+numpad_subtract&quot;,        &quot;command&quot;: &quot;editor.action.fontZoomOut&quot;    &#125;,    &#123;        &quot;key&quot;: &quot;f9&quot;,        &quot;command&quot;: &quot;workbench.action.tasks.runTask&quot;,        &quot;args&quot;: &quot;build&quot;    &#125;,    &#123;        &quot;key&quot;: &quot;f10&quot;,        &quot;command&quot;: &quot;workbench.action.tasks.runTask&quot;,        &quot;args&quot;: &quot;run&quot;    &#125;,    &#123;        &quot;key&quot;: &quot;f11&quot;,        &quot;command&quot;: &quot;workbench.action.tasks.runTask&quot;,        &quot;args&quot;: &quot;buildrun&quot;    &#125;]\n设置自动保存。\n&#123;    &quot;files.autoSave&quot;: &quot;afterDelay&quot;&#125;","tags":["VSCode"]}]